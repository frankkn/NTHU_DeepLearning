{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab10: Word2Vec\n",
    "<hr>\n",
    "\n",
    "110062802 呂宸漢\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup Tensorflow for GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # disable warning and info message\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib\n",
    "\n",
    "# Download the data.\n",
    "DOWNLOAD_URL = 'http://mattmahoney.net/dc/'\n",
    "DATA_FOLDER = \"data\"\n",
    "FILE_NAME = \"text8.zip\"\n",
    "EXPECTED_BYTES = 31344016\n",
    "\n",
    "\n",
    "def make_dir(path):\n",
    "    \"\"\" Create a directory if there isn't one already. \"\"\"\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "\n",
    "def download(file_name, expected_bytes):\n",
    "    \"\"\" Download the dataset text8 if it's not already downloaded \"\"\"\n",
    "    local_file_path = os.path.join(DATA_FOLDER, file_name)\n",
    "    if os.path.exists(local_file_path):\n",
    "        print(\"Dataset ready\")\n",
    "        return local_file_path\n",
    "    file_name, _ = urllib.request.urlretrieve(\n",
    "        os.path.join(DOWNLOAD_URL, file_name), local_file_path)\n",
    "    file_stat = os.stat(local_file_path)\n",
    "    if file_stat.st_size == expected_bytes:\n",
    "        print('Successfully downloaded the file', file_name)\n",
    "    else:\n",
    "        raise Exception(\n",
    "            'File ' + file_name +\n",
    "            ' might be corrupted. You should try downloading it with a browser.')\n",
    "    return local_file_path\n",
    "\n",
    "\n",
    "make_dir(DATA_FOLDER)\n",
    "file_path = download(FILE_NAME, EXPECTED_BYTES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 17005207\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "# Read the data into a list of strings.\n",
    "def read_data(file_path):\n",
    "    \"\"\" Read data into a list of tokens \"\"\"\n",
    "    with zipfile.ZipFile(file_path) as f:\n",
    "        # tf.compat.as_str() converts the input into string\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "\n",
    "\n",
    "vocabulary = read_data(file_path)\n",
    "print('Data size', len(vocabulary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# Build the dictionary and replace rare words with UNK token.\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\" Create two dictionaries and count of occuring words\n",
    "        - word_to_id: map of words to their codes\n",
    "        - id_to_word: maps codes to words (inverse word_to_id)\n",
    "        - count: map of words to count of occurrences\n",
    "    \"\"\"\n",
    "    # map unknown words to -1\n",
    "    count = [['UNK', -1]]\n",
    "    # count of occurences for words in vocabulary\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    word_to_id = dict()  # (word, id)\n",
    "    # record word id\n",
    "    for word, _ in count:\n",
    "        word_to_id[word] = len(word_to_id)\n",
    "    id_to_word = dict(\n",
    "        zip(word_to_id.values(), word_to_id.keys()))  # (id, word)\n",
    "    return word_to_id, id_to_word, count\n",
    "\n",
    "\n",
    "def convert_words_to_id(words, dictionary, count):\n",
    "    \"\"\" Replace each word in the dataset with its index in the dictionary \"\"\"\n",
    "    data_w2id = []\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        # return 0 if word is not in dictionary\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:\n",
    "            unk_count += 1\n",
    "        data_w2id.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    return data_w2id, count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Filling 4 global variables:\n",
    "# data_w2id - list of codes (integers from 0 to vocabulary_size-1).\n",
    "              This is the original text but words are replaced by their codes\n",
    "# count - map of words(strings) to count of occurrences\n",
    "# word_to_id - map of words(strings) to their codes(integers)\n",
    "# id_to_word - maps codes(integers) to words(strings)\n",
    "\"\"\"\n",
    "\n",
    "vocabulary_size = 50000\n",
    "word_to_id, id_to_word, count = build_dataset(vocabulary, vocabulary_size)\n",
    "data_w2id, count = convert_words_to_id(vocabulary, word_to_id, count)\n",
    "del vocabulary  # reduce memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# utility function\n",
    "def generate_sample(center_words, context_window_size):\n",
    "    \"\"\" Form training pairs according to the skip-gram model. \"\"\"\n",
    "    for idx, center in enumerate(center_words):\n",
    "        context = random.randint(1, context_window_size)\n",
    "        # get a random target before the center word\n",
    "        for target in center_words[max(0, idx - context): idx]:\n",
    "            yield center, target\n",
    "        # get a random target after the center word\n",
    "        for target in center_words[idx + 1: idx + context + 1]:\n",
    "            yield center, target\n",
    "\n",
    "\n",
    "def batch_generator(data, skip_window, batch_size):\n",
    "    \"\"\" Group a numeric stream into batches and yield them as Numpy arrays. \"\"\"\n",
    "    single_gen = generate_sample(data, skip_window)\n",
    "    while True:\n",
    "        center_batch = np.zeros(batch_size, dtype=np.int32)\n",
    "        target_batch = np.zeros([batch_size, 1], dtype=np.int32)\n",
    "        for idx in range(batch_size):\n",
    "            center_batch[idx], target_batch[idx] = next(single_gen)\n",
    "        yield center_batch, target_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Skip-gram word2vec model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some training settings\n",
    "training_steps = 80000\n",
    "skip_step = 2000\n",
    "\n",
    "# some hyperparameters\n",
    "batch_size = 512\n",
    "embed_size = 512\n",
    "num_sampled = 256\n",
    "learning_rate = 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Subclassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "# from tensorflow.python.keras.layers import Layer\n",
    "\n",
    "# embedding matrix - hidden layer\n",
    "class embedding_lookup(Layer):\n",
    "    def __init__(self):\n",
    "        super(embedding_lookup, self).__init__()\n",
    "        embedding_init = tf.keras.initializers.GlorotUniform()\n",
    "        self.embedding_matrix = self.add_weight(name=\"embedding_matrix\",\n",
    "                                                trainable=True,\n",
    "                                                shape=[vocabulary_size, embed_size],\n",
    "                                                initializer=embedding_init)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        center_words = inputs\n",
    "        embedding = tf.nn.embedding_lookup(self.embedding_matrix,\n",
    "                                           center_words,\n",
    "                                           name='embedding')\n",
    "        return embedding\n",
    "\n",
    "# context matrix - prediction layer\n",
    "class nce_loss(Layer):\n",
    "    def __init__(self):\n",
    "        super(nce_loss, self).__init__()\n",
    "        nce_w_init = tf.keras.initializers.TruncatedNormal(\n",
    "            stddev=1.0/(embed_size**0.5))\n",
    "        self.nce_weight = self.add_weight(name='nce_weight',\n",
    "                                          trainable=True,\n",
    "                                          shape=[vocabulary_size, embed_size],\n",
    "                                          initializer=nce_w_init)\n",
    "        self.nce_bias = self.add_weight(name='nce_bias',\n",
    "                                        trainable=True,\n",
    "                                        shape=[vocabulary_size],\n",
    "                                        initializer=tf.keras.initializers.Zeros)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        embedding, target_words = inputs[0], inputs[1]\n",
    "        loss = tf.reduce_mean(tf.nn.nce_loss(weights=self.nce_weight,\n",
    "                                             biases=self.nce_bias,\n",
    "                                             labels=target_words,\n",
    "                                             inputs=embedding,\n",
    "                                             num_sampled=num_sampled,\n",
    "                                             num_classes=vocabulary_size),\n",
    "                              name='loss')\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras import Model, Input\n",
    "# # from tensorflow.python.keras import Model, Input\n",
    "\n",
    "# center_words = Input(shape=(), name='center_words', dtype='int32')\n",
    "# target_words = Input(shape=(1), name='target_words', dtype='int32')\n",
    "\n",
    "# embedding = embedding_lookup()(center_words)\n",
    "# loss = nce_loss()((embedding, target_words))\n",
    "\n",
    "# word2vec = Model(name='word2vec',\n",
    "#                  inputs=[center_words, target_words],\n",
    "#                  outputs=[loss])\n",
    "# word2vec.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Subclassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "# from tensorflow.python.keras import Model\n",
    "\n",
    "class Word2Vec(Model):\n",
    "    def __init__(self):\n",
    "        '''To-Do: Define model variables'''\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.embedding_lookup = embedding_lookup()\n",
    "        self.loss = nce_loss()\n",
    "\n",
    "    def call(self, center_words, target_words):\n",
    "        '''To-Do: Define data flow and return loss'''\n",
    "        embedding = self.embedding_lookup(center_words)\n",
    "        loss = self.loss((embedding, target_words))\n",
    "        return loss\n",
    "\n",
    "word2vec = Word2Vec()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geneartor for `tf.data.Dataset`\n",
    "def gen():\n",
    "    \"\"\" Return a python generator that generates batches. \"\"\"\n",
    "    yield from batch_generator(data_w2id, 2, batch_size)\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(gen,\n",
    "                                         (tf.int32, tf.int32),\n",
    "                                         (tf.TensorShape([batch_size]), tf.TensorShape([batch_size, 1])))\\\n",
    "    .repeat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "optimizer = tf.keras.optimizers.SGD(\n",
    "    learning_rate=learning_rate, momentum=0.1, nesterov=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(center_words, target_words):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = word2vec(center_words, target_words)\n",
    "    gradients = tape.gradient(loss, word2vec.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, word2vec.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2000, Loss: 174.42\n",
      "Step 4000, Loss: 31.69\n",
      "Step 6000, Loss: 15.36\n",
      "Step 8000, Loss: 10.68\n",
      "Step 10000, Loss: 9.67\n",
      "Step 12000, Loss: 8.73\n",
      "Step 14000, Loss: 8.08\n",
      "Step 16000, Loss: 7.43\n",
      "Step 18000, Loss: 7.17\n",
      "Step 20000, Loss: 6.96\n",
      "Step 22000, Loss: 6.92\n",
      "Step 24000, Loss: 6.69\n",
      "Step 26000, Loss: 6.58\n",
      "Step 28000, Loss: 6.39\n",
      "Step 30000, Loss: 6.43\n",
      "Step 32000, Loss: 6.37\n",
      "Step 34000, Loss: 6.29\n",
      "Step 36000, Loss: 6.27\n",
      "Step 38000, Loss: 6.15\n",
      "Step 40000, Loss: 6.15\n",
      "Step 42000, Loss: 6.10\n",
      "Step 44000, Loss: 5.96\n",
      "Step 46000, Loss: 5.93\n",
      "Step 48000, Loss: 5.98\n",
      "Step 50000, Loss: 5.89\n"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "y = []\n",
    "for step, (center_words, target_words) in enumerate(dataset):\n",
    "    if step == training_steps:\n",
    "        break\n",
    "    train_step(center_words, target_words)\n",
    "\n",
    "    if ((step+1) % skip_step) == 0:\n",
    "        template = 'Step {:0}, Loss: {:.2f}'\n",
    "        x.append(step+1)\n",
    "        y.append(train_loss.result())\n",
    "        print(template.format(step+1, train_loss.result()))\n",
    "        train_loss.reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.plot(x, y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing the learned embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embedding matrix from model weights. > word2vec.weights[0]\n",
    "embedding_matrix = word2vec.weights[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_labels(low_dim_embs, labels):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "    plt.figure(figsize=(18, 18), dpi=150)  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(\n",
    "            label,\n",
    "            xy=(x, y),\n",
    "            xytext=(5, 2),\n",
    "            textcoords='offset points',\n",
    "            ha='right',\n",
    "            va='bottom')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(perplexity=30, n_components=2, init='pca',\n",
    "            n_iter=5000, method='exact', learning_rate='auto')\n",
    "plot_only = 400\n",
    "final_embeddings = embedding_matrix\n",
    "low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "labels = [id_to_word[i] for i in range(plot_only)]\n",
    "plot_with_labels(low_dim_embs, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.linalg as LA\n",
    "\n",
    "# handy method for calculating the similarity between 2 word\n",
    "def cos_sim(word1, word2):\n",
    "    id1 = word_to_id[word1]\n",
    "    id2 = word_to_id[word2]\n",
    "\n",
    "    vec1 = embedding_matrix[id1].numpy()\n",
    "    vec2 = embedding_matrix[id2].numpy()\n",
    "\n",
    "    return np.dot(vec1, vec2) / (LA.norm(vec1) * LA.norm(vec2))\n",
    "\n",
    "\n",
    "cos_sim('cat', 'dog'), cos_sim('man', 'woman')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_nearest(word, k):\n",
    "    vec = embedding_matrix[word_to_id[word]]\n",
    "\n",
    "    # calaulate cosine similarity  of `vec` and all other vocabularies\n",
    "    dot = np.dot(embedding_matrix.numpy(), vec)\n",
    "    embedding_norm = LA.norm(embedding_matrix.numpy(), axis=-1)\n",
    "    vec_norm = LA.norm(vec)\n",
    "    norm_product = embedding_norm * vec_norm\n",
    "    cos_sim = dot / norm_product\n",
    "\n",
    "    # print out top k nearest words\n",
    "    indices = np.argsort(cos_sim)[::-1][:k]\n",
    "    print('---top {} nearest words of {}---'.format(k, word))\n",
    "    for idx in indices:\n",
    "        print(id_to_word[idx])\n",
    "    print('\\n')\n",
    "\n",
    "top_k_nearest('five', 5)\n",
    "top_k_nearest('human', 5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "223e9936df39123efbb617b59821fa9903b74f50e310a538397c4da2f638e6c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
