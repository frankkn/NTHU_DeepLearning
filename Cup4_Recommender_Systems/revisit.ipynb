{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>DataLab Cup 4: Recommender Systems</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Platform: [Kaggle](https://www.kaggle.com/t/b06e248a3827434f80c4fdc6009d5fe0)\n",
    "\n",
    "Please download the dataset and the environment source code from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from evaluation.environment import TrainingEnvironment, TestingEnvironment\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# Check GPU\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        # Select GPU number 1\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], \"GPU\")\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Official hyperparameters for this competition (do not modify)\n",
    "N_TRAIN_USERS = 1000\n",
    "N_TEST_USERS = 2000\n",
    "N_ITEMS = 209527\n",
    "HORIZON = 2000\n",
    "TEST_EPISODES = 5\n",
    "SLATE_SIZE = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "USER_DATA = os.path.join('dataset', 'user_data.json')\n",
    "ITEM_DATA = os.path.join('dataset', 'item_data.json')\n",
    "\n",
    "# Output file path\n",
    "OUTPUT_PATH = os.path.join('output', 'output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[42558, 65272, 13353]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[146057, 195688, 143652]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[67551, 85247, 33714]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[116097, 192703, 103229]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[68756, 140123, 135289]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1995</td>\n",
       "      <td>[95090, 131393, 130239]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>[2360, 147130, 8145]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>[99794, 138694, 157888]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>[55561, 60372, 51442]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>[125409, 77906, 124792]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                   history\n",
       "0           0     [42558, 65272, 13353]\n",
       "1           1  [146057, 195688, 143652]\n",
       "2           2     [67551, 85247, 33714]\n",
       "3           3  [116097, 192703, 103229]\n",
       "4           4   [68756, 140123, 135289]\n",
       "...       ...                       ...\n",
       "1995     1995   [95090, 131393, 130239]\n",
       "1996     1996      [2360, 147130, 8145]\n",
       "1997     1997   [99794, 138694, 157888]\n",
       "1998     1998     [55561, 60372, 51442]\n",
       "1999     1999   [125409, 77906, 124792]\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user = pd.read_json(USER_DATA, lines=True)\n",
    "df_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>history</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>42558</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>65272</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>13353</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>146057</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>195688</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>60372</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>51442</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>125409</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>77906</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>124792</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id history  label\n",
       "0           0   42558      1\n",
       "0           0   65272      1\n",
       "0           0   13353      1\n",
       "1           1  146057      1\n",
       "1           1  195688      1\n",
       "...       ...     ...    ...\n",
       "1998     1998   60372      1\n",
       "1998     1998   51442      1\n",
       "1999     1999  125409      1\n",
       "1999     1999   77906      1\n",
       "1999     1999  124792      1\n",
       "\n",
       "[6000 rows x 3 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = df_user.explode(\"history\")\n",
    "pairs[\"label\"] = 1\n",
    "\n",
    "pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>headline</th>\n",
       "      <th>short_description</th>\n",
       "      <th>concat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>Health experts said it is too early to predict...</td>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>He was subdued by passengers and crew when he ...</td>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>\"Until you have a dog you don't understand wha...</td>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>Amy Cooper accused investment firm Franklin Te...</td>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209522</th>\n",
       "      <td>209522</td>\n",
       "      <td>RIM CEO Thorsten Heins' 'Significant' Plans Fo...</td>\n",
       "      <td>Verizon Wireless and AT&amp;T are already promotin...</td>\n",
       "      <td>RIM CEO Thorsten Heins' 'Significant' Plans Fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209523</th>\n",
       "      <td>209523</td>\n",
       "      <td>Maria Sharapova Stunned By Victoria Azarenka I...</td>\n",
       "      <td>Afterward, Azarenka, more effusive with the pr...</td>\n",
       "      <td>Maria Sharapova Stunned By Victoria Azarenka I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209524</th>\n",
       "      <td>209524</td>\n",
       "      <td>Giants Over Patriots, Jets Over Colts Among  M...</td>\n",
       "      <td>Leading up to Super Bowl XLVI, the most talked...</td>\n",
       "      <td>Giants Over Patriots, Jets Over Colts Among  M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209525</th>\n",
       "      <td>209525</td>\n",
       "      <td>Aldon Smith Arrested: 49ers Linebacker Busted ...</td>\n",
       "      <td>CORRECTION: An earlier version of this story i...</td>\n",
       "      <td>Aldon Smith Arrested: 49ers Linebacker Busted ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209526</th>\n",
       "      <td>209526</td>\n",
       "      <td>Dwight Howard Rips Teammates After Magic Loss ...</td>\n",
       "      <td>The five-time all-star center tore into his te...</td>\n",
       "      <td>Dwight Howard Rips Teammates After Magic Loss ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>209527 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        item_id                                           headline  \\\n",
       "0             0  Over 4 Million Americans Roll Up Sleeves For O...   \n",
       "1             1  American Airlines Flyer Charged, Banned For Li...   \n",
       "2             2  23 Of The Funniest Tweets About Cats And Dogs ...   \n",
       "3             3  The Funniest Tweets From Parents This Week (Se...   \n",
       "4             4  Woman Who Called Cops On Black Bird-Watcher Lo...   \n",
       "...         ...                                                ...   \n",
       "209522   209522  RIM CEO Thorsten Heins' 'Significant' Plans Fo...   \n",
       "209523   209523  Maria Sharapova Stunned By Victoria Azarenka I...   \n",
       "209524   209524  Giants Over Patriots, Jets Over Colts Among  M...   \n",
       "209525   209525  Aldon Smith Arrested: 49ers Linebacker Busted ...   \n",
       "209526   209526  Dwight Howard Rips Teammates After Magic Loss ...   \n",
       "\n",
       "                                        short_description  \\\n",
       "0       Health experts said it is too early to predict...   \n",
       "1       He was subdued by passengers and crew when he ...   \n",
       "2       \"Until you have a dog you don't understand wha...   \n",
       "3       \"Accidentally put grown-up toothpaste on my to...   \n",
       "4       Amy Cooper accused investment firm Franklin Te...   \n",
       "...                                                   ...   \n",
       "209522  Verizon Wireless and AT&T are already promotin...   \n",
       "209523  Afterward, Azarenka, more effusive with the pr...   \n",
       "209524  Leading up to Super Bowl XLVI, the most talked...   \n",
       "209525  CORRECTION: An earlier version of this story i...   \n",
       "209526  The five-time all-star center tore into his te...   \n",
       "\n",
       "                                                   concat  \n",
       "0       Over 4 Million Americans Roll Up Sleeves For O...  \n",
       "1       American Airlines Flyer Charged, Banned For Li...  \n",
       "2       23 Of The Funniest Tweets About Cats And Dogs ...  \n",
       "3       The Funniest Tweets From Parents This Week (Se...  \n",
       "4       Woman Who Called Cops On Black Bird-Watcher Lo...  \n",
       "...                                                   ...  \n",
       "209522  RIM CEO Thorsten Heins' 'Significant' Plans Fo...  \n",
       "209523  Maria Sharapova Stunned By Victoria Azarenka I...  \n",
       "209524  Giants Over Patriots, Jets Over Colts Among  M...  \n",
       "209525  Aldon Smith Arrested: 49ers Linebacker Busted ...  \n",
       "209526  Dwight Howard Rips Teammates After Magic Loss ...  \n",
       "\n",
       "[209527 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_item = pd.read_json(ITEM_DATA, lines=True)\n",
    "df_item['concat'] = df_item['headline'] + ' ' + df_item['short_description']\n",
    "df_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "has_enbedded = True\n",
    "\n",
    "if not has_enbedded:\n",
    "    model = SentenceTransformer('bert-base-nli-stsb-mean-tokens')\n",
    "    embeddings = model.encode(df_item['concat'].tolist())\n",
    "    \n",
    "    df_item['embedding'] = embeddings.tolist()\n",
    "    df_item['embedding'].to_pickle(\"./dataset/embedding.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>headline</th>\n",
       "      <th>short_description</th>\n",
       "      <th>concat</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>Health experts said it is too early to predict...</td>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>[0.504711925983429, -0.09660917520523071, 1.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>He was subdued by passengers and crew when he ...</td>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>[-0.7140856981277466, -0.020469149574637413, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>\"Until you have a dog you don't understand wha...</td>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>[0.3640383183956146, 0.46643778681755066, 0.22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>[-0.6239121556282043, 0.8828951716423035, 0.29...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>Amy Cooper accused investment firm Franklin Te...</td>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>[0.2773666977882385, 0.27412697672843933, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209522</th>\n",
       "      <td>209522</td>\n",
       "      <td>RIM CEO Thorsten Heins' 'Significant' Plans Fo...</td>\n",
       "      <td>Verizon Wireless and AT&amp;T are already promotin...</td>\n",
       "      <td>RIM CEO Thorsten Heins' 'Significant' Plans Fo...</td>\n",
       "      <td>[-0.4424550235271454, 0.4845919609069824, 0.63...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209523</th>\n",
       "      <td>209523</td>\n",
       "      <td>Maria Sharapova Stunned By Victoria Azarenka I...</td>\n",
       "      <td>Afterward, Azarenka, more effusive with the pr...</td>\n",
       "      <td>Maria Sharapova Stunned By Victoria Azarenka I...</td>\n",
       "      <td>[-0.2965202331542969, 0.0609898716211319, 0.14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209524</th>\n",
       "      <td>209524</td>\n",
       "      <td>Giants Over Patriots, Jets Over Colts Among  M...</td>\n",
       "      <td>Leading up to Super Bowl XLVI, the most talked...</td>\n",
       "      <td>Giants Over Patriots, Jets Over Colts Among  M...</td>\n",
       "      <td>[-0.05925000458955765, -0.564041018486023, 0.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209525</th>\n",
       "      <td>209525</td>\n",
       "      <td>Aldon Smith Arrested: 49ers Linebacker Busted ...</td>\n",
       "      <td>CORRECTION: An earlier version of this story i...</td>\n",
       "      <td>Aldon Smith Arrested: 49ers Linebacker Busted ...</td>\n",
       "      <td>[-0.25016430020332336, 0.15449568629264832, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209526</th>\n",
       "      <td>209526</td>\n",
       "      <td>Dwight Howard Rips Teammates After Magic Loss ...</td>\n",
       "      <td>The five-time all-star center tore into his te...</td>\n",
       "      <td>Dwight Howard Rips Teammates After Magic Loss ...</td>\n",
       "      <td>[-0.2558094561100006, -0.4093438386917114, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>209527 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        item_id                                           headline  \\\n",
       "0             0  Over 4 Million Americans Roll Up Sleeves For O...   \n",
       "1             1  American Airlines Flyer Charged, Banned For Li...   \n",
       "2             2  23 Of The Funniest Tweets About Cats And Dogs ...   \n",
       "3             3  The Funniest Tweets From Parents This Week (Se...   \n",
       "4             4  Woman Who Called Cops On Black Bird-Watcher Lo...   \n",
       "...         ...                                                ...   \n",
       "209522   209522  RIM CEO Thorsten Heins' 'Significant' Plans Fo...   \n",
       "209523   209523  Maria Sharapova Stunned By Victoria Azarenka I...   \n",
       "209524   209524  Giants Over Patriots, Jets Over Colts Among  M...   \n",
       "209525   209525  Aldon Smith Arrested: 49ers Linebacker Busted ...   \n",
       "209526   209526  Dwight Howard Rips Teammates After Magic Loss ...   \n",
       "\n",
       "                                        short_description  \\\n",
       "0       Health experts said it is too early to predict...   \n",
       "1       He was subdued by passengers and crew when he ...   \n",
       "2       \"Until you have a dog you don't understand wha...   \n",
       "3       \"Accidentally put grown-up toothpaste on my to...   \n",
       "4       Amy Cooper accused investment firm Franklin Te...   \n",
       "...                                                   ...   \n",
       "209522  Verizon Wireless and AT&T are already promotin...   \n",
       "209523  Afterward, Azarenka, more effusive with the pr...   \n",
       "209524  Leading up to Super Bowl XLVI, the most talked...   \n",
       "209525  CORRECTION: An earlier version of this story i...   \n",
       "209526  The five-time all-star center tore into his te...   \n",
       "\n",
       "                                                   concat  \\\n",
       "0       Over 4 Million Americans Roll Up Sleeves For O...   \n",
       "1       American Airlines Flyer Charged, Banned For Li...   \n",
       "2       23 Of The Funniest Tweets About Cats And Dogs ...   \n",
       "3       The Funniest Tweets From Parents This Week (Se...   \n",
       "4       Woman Who Called Cops On Black Bird-Watcher Lo...   \n",
       "...                                                   ...   \n",
       "209522  RIM CEO Thorsten Heins' 'Significant' Plans Fo...   \n",
       "209523  Maria Sharapova Stunned By Victoria Azarenka I...   \n",
       "209524  Giants Over Patriots, Jets Over Colts Among  M...   \n",
       "209525  Aldon Smith Arrested: 49ers Linebacker Busted ...   \n",
       "209526  Dwight Howard Rips Teammates After Magic Loss ...   \n",
       "\n",
       "                                                embedding  \n",
       "0       [0.504711925983429, -0.09660917520523071, 1.02...  \n",
       "1       [-0.7140856981277466, -0.020469149574637413, 0...  \n",
       "2       [0.3640383183956146, 0.46643778681755066, 0.22...  \n",
       "3       [-0.6239121556282043, 0.8828951716423035, 0.29...  \n",
       "4       [0.2773666977882385, 0.27412697672843933, -0.0...  \n",
       "...                                                   ...  \n",
       "209522  [-0.4424550235271454, 0.4845919609069824, 0.63...  \n",
       "209523  [-0.2965202331542969, 0.0609898716211319, 0.14...  \n",
       "209524  [-0.05925000458955765, -0.564041018486023, 0.5...  \n",
       "209525  [-0.25016430020332336, 0.15449568629264832, -0...  \n",
       "209526  [-0.2558094561100006, -0.4093438386917114, 0.0...  \n",
       "\n",
       "[209527 rows x 5 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists(\"./dataset/embedding.pkl\"):\n",
    "    embeddings_from_file = pd.read_pickle(\"./dataset/embedding.pkl\")\n",
    "    df_item['embedding'] = embeddings_from_file.tolist()\n",
    "\n",
    "df_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 計算相似性矩陣\n",
    "similarity_items = {}\n",
    "\n",
    "if not os.path.exists(\"./dataset/similarity_items.pkl\"):\n",
    "    # 提取 embedding 列的向量\n",
    "    embeddings = np.vstack(df_item['embedding'].to_numpy())\n",
    "\n",
    "    # 設定分批處理的大小\n",
    "    batch_size = 1000\n",
    "\n",
    "    for i in range(0, len(df_item), batch_size):\n",
    "        embeddings_batch = np.vstack(df_item['embedding'].iloc[i:i+batch_size].to_numpy())\n",
    "        similarity_matrix_batch = cosine_similarity(embeddings_batch, embeddings)\n",
    "\n",
    "        for item_id in range(i, min(i+batch_size, len(df_item))):\n",
    "            # 排序相似度，得到相似 item 的索引\n",
    "            similar_items = np.argsort(similarity_matrix_batch[item_id - i])[::-1][:100]\n",
    "\n",
    "            # 將相似 item 存入 similarity_items 字典中\n",
    "            similarity_items[df_item['item_id'].iloc[item_id]] = list(similar_items)\n",
    "\n",
    "    with open(\"./dataset/similarity_items.pkl\", \"wb\") as f:\n",
    "        pickle.dump(similarity_items, f)\n",
    "        \n",
    "else:\n",
    "    with open(\"./dataset/similarity_items.pkl\", \"rb\") as f:\n",
    "        similarity_items = pickle.load(f)\n",
    "\n",
    "# import pprint\n",
    "# pprint.pprint(similarity_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Environments\n",
    "\n",
    "We offer two simulation environments in this competition: `TrainingEnvironment` and `TestingEnvironment`. The only distinction between the two environments is the number of users, with 1000 for training and 2000 for testing. All public methods for both environments behave the same since they share the same base class.\n",
    "\n",
    "**Important Note: Ensure that you collect interaction data only by accessing the environment through the designated public methods listed below. Directly accessing or modifying any file or code in the `evaluation` directory, or retrieving internal attributes and states of the environment (including all attributes / methods starting with an underscore `_`), will be considered as cheating.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FunkSVDRecommender(tf.keras.Model):\n",
    "    '''\n",
    "    Simplified Funk-SVD recommender model\n",
    "    '''\n",
    "\n",
    "    def __init__(self, m_users: int, n_items: int, bias_mu, embedding_size: int, learning_rate: float,\n",
    "                 regularization_train: bool, regularization_update: bool, seed: int):\n",
    "        '''\n",
    "        Constructor of the model\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.m = m_users\n",
    "        self.n = n_items\n",
    "        self.k = embedding_size\n",
    "        self.lr = learning_rate\n",
    "        self.reg_train = regularization_train\n",
    "        self.reg_update = regularization_update\n",
    "        self.seed = seed\n",
    "        self.B_mu = tf.constant([bias_mu])\n",
    "\n",
    "        # user embeddings P\n",
    "        self.P = tf.Variable(tf.keras.initializers.RandomNormal()(shape=(self.m, self.k)))\n",
    "\n",
    "        # item embeddings Q\n",
    "        self.Q = tf.Variable(tf.keras.initializers.RandomNormal()(shape=(self.n, self.k)))\n",
    "        \n",
    "        # bias term\n",
    "        self.B_user = tf.Variable(tf.keras.initializers.RandomNormal(seed=self.seed)(shape=(self.m, 1)))\n",
    "        self.B_item = tf.Variable(tf.keras.initializers.RandomNormal(seed=self.seed)(shape=(self.n, 1)))\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = tf.optimizers.Adam(learning_rate=self.lr)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, user_ids: tf.Tensor, item_ids: tf.Tensor) -> tf.Tensor:\n",
    "        '''\n",
    "        Forward pass used in training and validating\n",
    "        '''\n",
    "        # dot product the user and item embeddings corresponding to the observed interaction pairs to produce predictions\n",
    "        y_pred = tf.reduce_sum(tf.gather(self.P, indices=user_ids) * tf.gather(self.Q, indices=item_ids), axis=1)\n",
    "        \n",
    "        y_pred = tf.add(y_pred, tf.squeeze(tf.gather(self.B_user, indices=user_ids)))\n",
    "        y_pred = tf.add(y_pred, tf.squeeze(tf.gather(self.B_item, indices=item_ids)))\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    @tf.function\n",
    "    def compute_loss(self, y_true: tf.Tensor, y_pred: tf.Tensor, regularization: bool) -> tf.Tensor:\n",
    "        '''\n",
    "        Compute the MSE loss of the model\n",
    "        '''\n",
    "        # loss = tf.losses.binary_crossentropy(y_true, y_pred, from_logits=True)\n",
    "        \n",
    "        if regularization:\n",
    "            loss = tf.losses.binary_crossentropy(y_true, y_pred)\n",
    "            # loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "            reg = 0.01 * (tf.nn.l2_loss(self.Q) + tf.nn.l2_loss(self.P) +\n",
    "                          tf.nn.l2_loss(self.B_item) + tf.nn.l2_loss(self.B_user))\n",
    "            loss += reg\n",
    "        else:\n",
    "            loss = tf.losses.binary_crossentropy(y_true, y_pred)\n",
    "            # loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, data: tf.Tensor) -> tf.Tensor:\n",
    "        '''\n",
    "        Train the model with one batch\n",
    "        data: batched user-item interactions\n",
    "        each record in data is in the format [user_id, history, clicked]\n",
    "        '''\n",
    "#         user_ids = tf.cast(data[:, 0], dtype=tf.int32)\n",
    "#         item_ids = tf.cast(data[:, 1], dtype=tf.int32)\n",
    "#         y_true = tf.cast(data[:, 2], dtype=tf.float32)\n",
    "\n",
    "        user_ids, item_ids, y_true = data\n",
    "    \n",
    "#         print(f\"user_ids shape: {user_ids.shape}\")\n",
    "#         print(f\"item_ids shape: {item_ids.shape}\")\n",
    "\n",
    "        # compute loss\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(user_ids, item_ids)\n",
    "            loss = self.compute_loss(y_true, y_pred, self.reg_train)\n",
    "\n",
    "        # compute gradients\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "\n",
    "        # update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @tf.function\n",
    "    def eval_predict_onestep(self, user_id: int) -> tf.Tensor:\n",
    "        '''\n",
    "        Retrieve and return the NewsIDs of the 5 recommended news given a query\n",
    "        You should return a tf.Tensor with shape=(5,)\n",
    "        '''\n",
    "        user_id = tf.cast(user_id, tf.int32)\n",
    "        \n",
    "        # dot product the selected user and all item embeddings to produce predictions\n",
    "        y_pred = tf.reduce_sum(tf.gather(self.P, user_id) * self.Q, axis=1)\n",
    "        \n",
    "        y_pred = tf.add(tf.gather(self.B_user, user_id), y_pred)\n",
    "        y_pred = tf.add(tf.squeeze(self.B_item), y_pred)\n",
    "\n",
    "        # select the top 5 items with highest scores in y_pred\n",
    "        y_top_5 = tf.math.top_k(y_pred, k=5).indices\n",
    "\n",
    "        return y_top_5\n",
    "    \n",
    "    def get_topk(self, user_id, k=5) -> tf.Tensor:\n",
    "        user_ids = tf.repeat(tf.constant(user_id), self.n)\n",
    "        item_ids = tf.range(self.n)\n",
    "        rank_list = tf.squeeze(self.call(user_ids, item_ids))\n",
    "        return tf.math.top_k(rank_list, k=k).indices.numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_topk(clicked_id, k=2, choose_self=True):\n",
    "    n = 0 if choose_self else 1\n",
    "    if clicked_id in similarity_items:\n",
    "        return similarity_items[clicked_id][n : n + k]\n",
    "\n",
    "    item_to_embedding = embeddings_from_file\n",
    "    scores = tf.losses.CosineSimilarity(reduction=\"none\")(\n",
    "        tf.repeat(\n",
    "            tf.constant(item_to_embedding.iloc[clicked_id], shape=(1, 384)),\n",
    "            len(item_to_embedding),\n",
    "            axis=0,\n",
    "        ),\n",
    "        tf.constant(item_to_embedding),\n",
    "    )\n",
    "\n",
    "    sort_items = tf.argsort(scores).numpy().tolist()\n",
    "\n",
    "    similarity_items[clicked_id] = sort_items[:100]\n",
    "    return sort_items[n : n + k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "EMBEDDING_SIZE = 256\n",
    "BATCH_SIZE = 256\n",
    "N_EPOCHS = 10\n",
    "LEARNING_RATE = 5*(1e-3)\n",
    "SEED = 514\n",
    "BIAS_MU = 1.0\n",
    "REGULARIZATION_TRAIN = True\n",
    "REGULARIZATION_UPDATE = True\n",
    "\n",
    "N_NEG = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare datasets\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((\n",
    "            tf.convert_to_tensor(pairs[\"user_id\"].to_numpy(dtype=int)),\n",
    "            tf.convert_to_tensor(pairs[\"history\"].to_numpy(dtype=int)),\n",
    "            tf.convert_to_tensor(pairs[\"label\"].to_numpy(dtype=int)),\n",
    "        ))\n",
    "\n",
    "dataset_train = dataset_train.batch(batch_size=BATCH_SIZE, num_parallel_calls=tf.data.AUTOTUNE).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "model = FunkSVDRecommender(m_users=N_TRAIN_USERS, \n",
    "                           n_items=N_ITEMS,\n",
    "                           bias_mu=BIAS_MU,\n",
    "                           embedding_size=EMBEDDING_SIZE, \n",
    "                           learning_rate=LEARNING_RATE,\n",
    "                           regularization_train=REGULARIZATION_TRAIN,\n",
    "                           regularization_update=REGULARIZATION_UPDATE,\n",
    "                           seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "ckpt_manager = tf.train.CheckpointManager(checkpoint, \"./checkpoint/FunkSVD\", max_to_keep=5)\n",
    "best_manager = tf.train.CheckpointManager(checkpoint, \"./checkpoint/FunkSVD/best\", max_to_keep=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, n_neg=5):\n",
    "    epoch_loss = []\n",
    "    \n",
    "    pbar = trange(N_EPOCHS, desc=\"Training\", ncols=0)\n",
    "    for _ in pbar:\n",
    "        batch_loss = []\n",
    "        \n",
    "        for user_ids, pos_item_ids, labels in dataset:\n",
    "            losses = []\n",
    "            batch_size = len(user_ids)\n",
    "            \n",
    "            # Train positive samples\n",
    "            loss = model.train_step((\n",
    "                user_ids,\n",
    "                pos_item_ids,\n",
    "                labels,\n",
    "            ))\n",
    "            losses.append(loss)\n",
    "\n",
    "            # Train negative samples\n",
    "            neg_item_ids = tf.random.uniform(\n",
    "                shape=(n_neg, batch_size),\n",
    "                minval=0,\n",
    "                maxval=N_ITEMS,\n",
    "                dtype=tf.int32,\n",
    "            )\n",
    "            for _neg_item_id in neg_item_ids:\n",
    "                loss = model.train_step((\n",
    "                    tf.constant(user_ids),\n",
    "                    tf.constant(_neg_item_id),\n",
    "                    tf.zeros(batch_size),\n",
    "                ))\n",
    "                losses.append(loss)\n",
    "\n",
    "            batch_loss.append(tf.reduce_mean(losses).numpy())\n",
    "        epoch_loss.append(np.mean(batch_loss))\n",
    "        pbar.set_postfix({\"loss\": epoch_loss[-1]})\n",
    "    pbar.set_postfix({\"loss\": np.mean(epoch_loss)}, refresh=True)\n",
    "\n",
    "    return model, np.mean(epoch_loss)\n",
    "\n",
    "\n",
    "def update(model, user_id, clicked_id):\n",
    "    # Positive samples\n",
    "    model.train_step((\n",
    "        tf.convert_to_tensor([[user_id]]),\n",
    "        tf.convert_to_tensor([[clicked_id]]),\n",
    "        tf.ones(1),\n",
    "    ))\n",
    "\n",
    "    # Negative samples\n",
    "    neg_item_ids = tf.random.uniform(\n",
    "        shape=(N_NEG,),\n",
    "        minval=0,\n",
    "        maxval=N_ITEMS,\n",
    "        dtype=tf.int32,\n",
    "    )\n",
    "    model.train_step((\n",
    "        tf.repeat(user_id, N_NEG),\n",
    "        neg_item_ids,\n",
    "        tf.zeros(N_NEG),\n",
    "    ))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Explore pipeline\n",
    "def explore(env, model, pairs, slate_size=5):\n",
    "    hit_count = 0\n",
    "    pbar = tqdm(desc=\"Explore\")\n",
    "    while env.has_next_state():\n",
    "        user_id = env.get_state()\n",
    "        random_pos_item_id = random.choice(tuple(df_user.loc[user_id, 'history']))\n",
    "        coll_slate = model.get_topk(user_id, 2)\n",
    "        cont_slate = get_content_topk(random_pos_item_id, 3, False)\n",
    "        slate = np.unique(coll_slate + cont_slate).tolist()\n",
    "        while len(slate) < slate_size:\n",
    "            slate = np.unique(\n",
    "                slate\n",
    "                + random.sample(model.get_topk(user_id, 10), slate_size - len(slate))\n",
    "            ).tolist()\n",
    "        clicked_id, _ = env.get_response(slate)\n",
    "\n",
    "        if clicked_id != -1:\n",
    "            hit_count += 1\n",
    "            new_row = pd.DataFrame({'user_id': user_id, 'history':clicked_id, 'label': 1}, index=[0])\n",
    "            pairs = pd.concat([pairs, new_row], ignore_index=True)\n",
    "            model = update(model, user_id, clicked_id)\n",
    "\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix({\"#click\": hit_count})\n",
    "\n",
    "    return model, hit_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eposide 1 starts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explore: 2800it [00:47, 59.10it/s, #click=298]\n",
      "Training: 100% 10/10 [00:17<00:00,  1.74s/it, loss=0.701]\n",
      "Explore: 5932it [01:49, 53.95it/s, #click=572]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Score: 0.002966\n",
      "Best model saved at ./checkpoint/FunkSVD/best\\ckpt-32.\n",
      "==============================================================\n",
      " Eposide 2 starts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 10/10 [00:33<00:00,  3.34s/it, loss=0.895]\n",
      "Explore: 5923it [01:50, 53.83it/s, #click=566]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Score: 0.002962\n",
      "==============================================================\n",
      " Eposide 3 starts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 10/10 [00:14<00:00,  1.42s/it, loss=0.805]\n",
      "Explore: 5912it [00:59, 99.21it/s, #click=560] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Score: 0.002956\n",
      "==============================================================\n",
      " Eposide 4 starts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 10/10 [00:14<00:00,  1.43s/it, loss=0.78]\n",
      "Explore: 5991it [01:01, 97.44it/s, #click=604] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Score: 0.002996\n",
      "Best model saved at ./checkpoint/FunkSVD/best\\ckpt-36.\n",
      "==============================================================\n",
      " Eposide 5 starts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 10/10 [00:14<00:00,  1.44s/it, loss=0.76]\n",
      "Explore: 5974it [01:00, 98.37it/s, #click=593] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Score: 0.002987\n",
      "==============================================================\n",
      " Eposide 6 starts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 10/10 [00:14<00:00,  1.42s/it, loss=0.762]\n",
      "Explore: 5984it [01:00, 98.17it/s, #click=603] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Score: 0.002992\n",
      "==============================================================\n",
      " Eposide 7 starts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 10/10 [00:14<00:00,  1.42s/it, loss=0.792]\n",
      "Explore: 5967it [01:00, 97.89it/s, #click=597] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Score: 0.002984\n",
      "==============================================================\n",
      " Eposide 8 starts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 10/10 [00:14<00:00,  1.42s/it, loss=0.781]\n",
      "Explore: 6020it [01:06, 90.72it/s, #click=627] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Score: 0.003010\n",
      "Best model saved at ./checkpoint/FunkSVD/best\\ckpt-41.\n",
      "==============================================================\n",
      " Eposide 9 starts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 10/10 [00:14<00:00,  1.48s/it, loss=0.793]\n",
      "Explore: 5991it [01:05, 90.95it/s, #click=613] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Score: 0.002996\n",
      "==============================================================\n",
      " Eposide 10 starts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 10/10 [00:15<00:00,  1.50s/it, loss=0.793]\n",
      "Explore: 5977it [01:04, 92.39it/s, #click=600] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Score: 0.002989\n",
      "==============================================================\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    print(f\" Eposide {epoch} starts...\")\n",
    "\n",
    "    # Initialize\n",
    "    env = TrainingEnvironment()\n",
    "    \n",
    "    # training\n",
    "    model, _ = train(model, dataset_train, n_neg=N_NEG)\n",
    "          \n",
    "    # Explore and update    \n",
    "    model, _ = explore(env, model, pairs, 5)\n",
    "    score = np.mean(env.get_score())\n",
    "    print(f\"Avg. Score: {score:.6f}\")\n",
    "\n",
    "    # Save\n",
    "    ckpt_manager.save()\n",
    "    \n",
    "    df_user.to_pickle(\"./dataset/user_data_plus.pkl\")\n",
    "    with open(\"./dataset/similarity_items.pkl\", \"wb\") as f:\n",
    "        pickle.dump(similarity_items, f)\n",
    "\n",
    "    # Save best model\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_manager.save()\n",
    "        print(f\"Best model saved at {best_manager.latest_checkpoint}.\")\n",
    "    print(\"==============================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class History:\n",
    "    def __init__(self, user_path):\n",
    "        df_user = pd.read_json(user_path, lines=True)\n",
    "        self.init_histories = df_user.set_index(\"user_id\")[\"history\"]\n",
    "        self.curr_histories = self.init_histories.copy()\n",
    "\n",
    "    def reset(self):\n",
    "        self.curr_histories = self.init_histories.copy()\n",
    "\n",
    "    def add(self, user_id, item_id):\n",
    "        self.curr_histories.loc[user_id].append(item_id)\n",
    "\n",
    "    def get(self, user_id):\n",
    "        return self.curr_histories.loc[user_id]\n",
    "\n",
    "    def update_init(self, sequence):\n",
    "        self.init_histories = (\n",
    "            pd.DataFrame(sequence, columns=[\"user_id\", \"history\"])\n",
    "            .groupby(\"user_id\")[\"history\"]\n",
    "            .apply(list)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored from ./checkpoint/FunkSVD/best\\ckpt-41.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 12358it [02:13, 92.23it/s, #click=1503] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored from ./checkpoint/FunkSVD/best\\ckpt-41.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 12324it [02:08, 95.91it/s, #click=1489] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored from ./checkpoint/FunkSVD/best\\ckpt-41.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 12316it [02:07, 96.80it/s, #click=1482] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored from ./checkpoint/FunkSVD/best\\ckpt-41.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 12332it [02:01, 101.71it/s, #click=1494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored from ./checkpoint/FunkSVD/best\\ckpt-41.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 12407it [02:02, 100.95it/s, #click=1548]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>avg_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1995</td>\n",
       "      <td>0.0025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>0.0025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>0.0025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>0.0025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>0.0025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id  avg_score\n",
       "0           0     0.0025\n",
       "1           1     0.0025\n",
       "2           2     0.0030\n",
       "3           3     0.0025\n",
       "4           4     0.0025\n",
       "...       ...        ...\n",
       "1995     1995     0.0025\n",
       "1996     1996     0.0025\n",
       "1997     1997     0.0025\n",
       "1998     1998     0.0025\n",
       "1999     1999     0.0025\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_ckpt_dir = \"./checkpoint/FunkSVD/best\"\n",
    "\n",
    "# Initialize the testing environment\n",
    "test_env = TestingEnvironment()\n",
    "scores = []\n",
    "\n",
    "# Repeat the testing process for 5 times\n",
    "for epoch in range(5):\n",
    "    # [TODO] Load your model weights here (in the beginning of each testing episode)\n",
    "    # [TODO] Code for loading your model weights...\n",
    "    \n",
    "    print(f\"Model restored from {tf.train.latest_checkpoint(best_ckpt_dir)}.\")\n",
    "    checkpoint = tf.train.Checkpoint(model=model)\n",
    "    checkpoint.restore(tf.train.latest_checkpoint(best_ckpt_dir))\n",
    "    history = History(\"./dataset/user_data.json\")\n",
    "    clicked_count = 0\n",
    "\n",
    "    # Start the testing process\n",
    "    with tqdm(desc=\"Testing\") as pbar:\n",
    "        # Run as long as there exist some active users\n",
    "        while test_env.has_next_state():\n",
    "            # Get the current user id\n",
    "            cur_user = test_env.get_state()\n",
    "\n",
    "            # [TODO] Employ your recommendation policy to generate a slate of 5 distinct items\n",
    "            # [TODO] Code for generating the recommended slate...\n",
    "            random_pos_item_id = random.choice(\n",
    "                np.unique(history.get(cur_user)).tolist()\n",
    "            )\n",
    "            coll_slate = model.get_topk(cur_user, 2)\n",
    "            cont_slate = get_content_topk(random_pos_item_id, 3, False)\n",
    "            slate = np.unique(coll_slate + cont_slate).tolist()\n",
    "\n",
    "            while len(slate) < 5:\n",
    "                slate = np.unique(\n",
    "                    slate\n",
    "                    + random.sample(\n",
    "                        model.get_topk(cur_user, 10),\n",
    "                        5 - len(slate),\n",
    "                    )\n",
    "                ).tolist()\n",
    "\n",
    "            # Get the response of the slate from the environment\n",
    "            clicked_id, _in_environment = test_env.get_response(slate)\n",
    "\n",
    "            # [TODO] Update your model here (optional)\n",
    "            # [TODO] You can update your model at each step, or perform a batched update after some interval\n",
    "            # [TODO] Code for updating your model...\n",
    "            if clicked_id != -1:\n",
    "                clicked_count += 1\n",
    "                history.add(cur_user, clicked_id)\n",
    "                model = update(model, cur_user, clicked_id)\n",
    "                pbar.set_postfix({\"#click\": clicked_count})\n",
    "\n",
    "            # Update the progress indicator\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Record the score of this testing episode\n",
    "    scores.append(test_env.get_score())\n",
    "\n",
    "    # Reset the testing environment\n",
    "    test_env.reset()\n",
    "\n",
    "    # [TODO] Delete or reset your model weights here (in the end of each testing episode)\n",
    "    # [TODO] Code for deleting your model weights...\n",
    "    checkpoint.restore(tf.train.latest_checkpoint(best_ckpt_dir))\n",
    "    history.reset()\n",
    "\n",
    "# Calculate the average scores\n",
    "avg_scores = [np.average(score) for score in zip(*scores)]\n",
    "\n",
    "# Generate a DataFrame to output the result in a .csv file\n",
    "df_result = pd.DataFrame([[user_id, avg_score] for user_id, avg_score in enumerate(avg_scores)],columns=[\"user_id\", \"avg_score\"],)\n",
    "df_result.to_csv(OUTPUT_PATH, index=False)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Model使用FunkSVDRecommender，是一種Collaborative Filtering的推薦系統，主要是利用user和item的interactions，目標是預測用戶對尚未互動過的物品的興趣，以便向用戶推薦他們可能感興趣的物品。其作法是建立一個User-Item Interaction Matrix，並將這個矩陣分解為三個矩陣的乘積，(User Matrix, Item Matrix, Diagonal Matrix)，再把對角矩陣開根號分別乘進去前兩個矩陣，即可得到${R} = {P}_{mxk} Q^T_{kxn}$。\n",
    "\n",
    "挑選這個Model主要是因為如下原因:\n",
    "1. Implicit Feature Representation: FunkSVD 將用戶和物品表示為具有隱性特徵的向量，這些特徵是model自動學習的。這使得model能捕捉到用戶和物品之間的複雜關係。\n",
    "2. 簡單且易於實現，通常能夠在大規模數據集上取得不錯的效果。\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "1. Data collecting:利用Training environment去取得更多筆的user data，進而去推測前1000筆user的興趣。\n",
    "2. Hyperparameters tuning:主要是tune `LEARNING_RATE`跟`N_EPOCHS`，最後是使用1e-3和200。\n",
    "3. Training process:主要就是不斷在環境裡跟使用者互動，並更新model權重。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussions\n",
    "\n",
    "1. 由於 user_data.json 的資料量只有6000個iteraction，必須透過執行 TrainingEnvironment收集多一點interaction data，否則可能會比 random還差。\n",
    "2. 由於物品數量龐大，大部分的互動都屬於負面互動（未點擊）。同時，系統中存在隨機性（對於同一個使用者-物品組合，有時會發生點擊，有時則不會）。如果將互動數據用1和-1表示正面和負面互動，可能會引入過多的Noise。可能要使用一些soft label，例如給一個使用者多看幾次同樣的物品，再取其點擊的平均並標準化等。\n",
    "3. 多取得與Training user的互動(類似於hack模擬環境的user data)，並從大部分人的趨勢去推測某些熱門新聞或是user的興趣，例如:對於同一則新聞是否會再次點擊，也許看過的就不會想再看等等。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
