{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad623b1a",
   "metadata": {},
   "source": [
    "# Lab12-2: Image Captioning\n",
    "\n",
    "In the last Lab, use a combination of convolutional neural networks to obtain the vectorial representation of images and recurrent neural networks to decode those representations into natural language sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3f1a216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ab97a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn includes many helpful utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Input, layers, Model\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79daf615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140000\n",
      "./words_captcha/a0.png\n",
      "<start> t h u s <end>\n"
     ]
    }
   ],
   "source": [
    "# Store captions and image names in vectors\n",
    "all_captions = []\n",
    "all_img_name_vector = [] # 140000 images\n",
    "\n",
    "with open('./words_captcha/spec_train_val.txt') as f:\n",
    "    for line in f:\n",
    "        img_name, caption = line.strip().split()\n",
    "        all_img_name_vector.append(f'./words_captcha/{img_name}.png')\n",
    "        all_captions.append('<start> ' + ' '.join(caption) + ' <end>')\n",
    "        \n",
    "for i in range(120000, 140000):\n",
    "    all_img_name_vector.append(f'./words_captcha/a{i}.png')\n",
    "    \n",
    "print(len(all_img_name_vector)) # 140000\n",
    "print(all_img_name_vector[0])\n",
    "print(all_captions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bd139e",
   "metadata": {},
   "source": [
    "## Preprocess and tokenize the captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80ac5ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum length of any caption in our dataset\n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fa8c808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> t h u s <end>\n",
      "[2, 9, 18, 17, 6, 3]\n"
     ]
    }
   ],
   "source": [
    "# Choose the top 5000 words from the vocabulary\n",
    "top_k = 5000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "\n",
    "# Create the tokenized vectors\n",
    "train_seqs = tokenizer.texts_to_sequences(all_captions)\n",
    "\n",
    "print(all_captions[0])\n",
    "print(train_seqs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "946e415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afe57809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  9 18 17  6  3  0]\n"
     ]
    }
   ],
   "source": [
    "# Pad each vector to the max_length of the captions\n",
    "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "print(cap_vector[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7d962bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# Calculates the max_length, which is used to store the attention weights\n",
    "max_length = calc_max_length(train_seqs)\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6c0721",
   "metadata": {},
   "source": [
    "## Split the data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fcab839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle captions and image_names together\n",
    "# Set a random state\n",
    "# img_name_train, caption_train = shuffle(all_img_name_vector[:100000], cap_vector[:100000], random_state=514)\n",
    "# img_name_valid, caption_valid = shuffle(all_img_name_vector[100000:120000], cap_vector[100000:120000], random_state=514)\n",
    "\n",
    "img_name_train, cap_train = all_img_name_vector[:100000], cap_vector[:100000]\n",
    "img_name_valid, cap_val = all_img_name_vector[100000:120000], cap_vector[100000:120000]\n",
    "\n",
    "img_name_test = all_img_name_vector[120000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593f2843",
   "metadata": {},
   "source": [
    "## Create a tf.data dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c39d0e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "BUFFER_SIZE = 5000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "num_steps = len(img_name_train) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d31f8a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (200, 300)\n",
    "\n",
    "def load_image(image_path, cap):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, IMAGE_SIZE)\n",
    "    img = img / 255 * 2 - 1\n",
    "    # img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3cff259",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\\\n",
    "                               .map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "                               .shuffle(BUFFER_SIZE)\\\n",
    "                               .batch(BATCH_SIZE, drop_remainder=True)\\\n",
    "                               .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset_valid = tf.data.Dataset.from_tensor_slices((img_name_valid, cap_val))\\\n",
    "                               .map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "                               .batch(BATCH_SIZE, drop_remainder=True)\\\n",
    "                               .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea08df2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (100, 200, 300, 3)\n",
      "Caption: [ 2 14  8  7  3  0  0]\n"
     ]
    }
   ],
   "source": [
    "for img, cap in dataset_valid.take(1):\n",
    "    # 印第一個pair的info\n",
    "    print(\"Image shape:\", img.shape)\n",
    "    print(\"Caption:\", cap[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822e89dd",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "506aa009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "#                                                 weights='imagenet')\n",
    "# new_input = image_model.input\n",
    "# hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "# image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb3c695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_leaky_relu(inputs, filters, size, stride):\n",
    "    x = layers.Conv2D(filters, size, stride, padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.1)(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "565500f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"YOLO\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 200, 300, 3)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 100, 150, 64)      9472      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 100, 150, 64)     256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 100, 150, 64)      0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 50, 75, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 50, 75, 192)       110784    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 50, 75, 192)      768       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 50, 75, 192)       0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 25, 37, 192)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 25, 37, 128)       24704     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 25, 37, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 25, 37, 128)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 25, 37, 256)       295168    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 25, 37, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 25, 37, 256)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 25, 37, 256)       65792     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 25, 37, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 25, 37, 256)       0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 25, 37, 512)       1180160   \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 25, 37, 512)      2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 25, 37, 512)       0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 12, 18, 512)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 12, 18, 256)       131328    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 12, 18, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 12, 18, 256)       0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 12, 18, 512)       1180160   \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 12, 18, 512)      2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 12, 18, 512)       0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 12, 18, 256)       131328    \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 12, 18, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_8 (LeakyReLU)   (None, 12, 18, 256)       0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 12, 18, 512)       1180160   \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 12, 18, 512)      2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_9 (LeakyReLU)   (None, 12, 18, 512)       0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 12, 18, 256)       131328    \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 12, 18, 256)      1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_10 (LeakyReLU)  (None, 12, 18, 256)       0         \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 12, 18, 512)       1180160   \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 12, 18, 512)      2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_11 (LeakyReLU)  (None, 12, 18, 512)       0         \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 12, 18, 256)       131328    \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 12, 18, 256)      1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_12 (LeakyReLU)  (None, 12, 18, 256)       0         \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 12, 18, 512)       1180160   \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 12, 18, 512)      2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_13 (LeakyReLU)  (None, 12, 18, 512)       0         \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 12, 18, 512)       262656    \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 12, 18, 512)      2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_14 (LeakyReLU)  (None, 12, 18, 512)       0         \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 12, 18, 1024)      4719616   \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 12, 18, 1024)     4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " leaky_re_lu_15 (LeakyReLU)  (None, 12, 18, 1024)      0         \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 6, 9, 1024)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 6, 9, 512)         524800    \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 6, 9, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_16 (LeakyReLU)  (None, 6, 9, 512)         0         \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 6, 9, 1024)        4719616   \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 6, 9, 1024)       4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_17 (LeakyReLU)  (None, 6, 9, 1024)        0         \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 6, 9, 512)         524800    \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 6, 9, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_18 (LeakyReLU)  (None, 6, 9, 512)         0         \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 6, 9, 1024)        4719616   \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 6, 9, 1024)       4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_19 (LeakyReLU)  (None, 6, 9, 1024)        0         \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 6, 9, 1024)        9438208   \n",
      "                                                                 \n",
      " batch_normalization_20 (Bat  (None, 6, 9, 1024)       4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_20 (LeakyReLU)  (None, 6, 9, 1024)        0         \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 3, 5, 1024)        9438208   \n",
      "                                                                 \n",
      " batch_normalization_21 (Bat  (None, 3, 5, 1024)       4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_21 (LeakyReLU)  (None, 3, 5, 1024)        0         \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 3, 5, 1024)        9438208   \n",
      "                                                                 \n",
      " batch_normalization_22 (Bat  (None, 3, 5, 1024)       4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_22 (LeakyReLU)  (None, 3, 5, 1024)        0         \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 3, 5, 1024)        9438208   \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, 3, 5, 1024)       4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_23 (LeakyReLU)  (None, 3, 5, 1024)        0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,208,704\n",
      "Trainable params: 60,182,336\n",
      "Non-trainable params: 26,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_inputs = keras.Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "x = conv_leaky_relu(img_inputs, 64, 7, 2)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = conv_leaky_relu(x, 192, 3, 1)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = conv_leaky_relu(x, 128, 1, 1)\n",
    "x = conv_leaky_relu(x, 256, 3, 1)\n",
    "x = conv_leaky_relu(x, 256, 1, 1)\n",
    "x = conv_leaky_relu(x, 512, 3, 1)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = conv_leaky_relu(x, 256, 1, 1)\n",
    "x = conv_leaky_relu(x, 512, 3, 1)\n",
    "x = conv_leaky_relu(x, 256, 1, 1)\n",
    "x = conv_leaky_relu(x, 512, 3, 1)\n",
    "x = conv_leaky_relu(x, 256, 1, 1)\n",
    "x = conv_leaky_relu(x, 512, 3, 1)\n",
    "x = conv_leaky_relu(x, 256, 1, 1)\n",
    "x = conv_leaky_relu(x, 512, 3, 1)\n",
    "x = conv_leaky_relu(x, 512, 1, 1)\n",
    "x = conv_leaky_relu(x, 1024, 3, 1)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = conv_leaky_relu(x, 512, 1, 1)\n",
    "x = conv_leaky_relu(x, 1024, 3, 1)\n",
    "x = conv_leaky_relu(x, 512, 1, 1)\n",
    "x = conv_leaky_relu(x, 1024, 3, 1)\n",
    "x = conv_leaky_relu(x, 1024, 3, 1)\n",
    "x = conv_leaky_relu(x, 1024, 3, 2)\n",
    "x = conv_leaky_relu(x, 1024, 3, 1)\n",
    "x = conv_leaky_relu(x, 1024, 3, 1)\n",
    "\n",
    "YOLO = keras.Model(inputs=img_inputs, outputs=x, name=\"YOLO\")\n",
    "\n",
    "YOLO.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "391e7f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 15, embedding_dim)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, 10, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # attention_weights shape == (batch_size, 15, 1)\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94cb1ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 15, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "112bfc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "\n",
    "        # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b099b9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c262f967",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fec5a4a",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ef3cc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './checkpoints/YOLO/'\n",
    "ckpt = tf.train.Checkpoint(feature_extractor = YOLO,\n",
    "                           encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e82bff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33a3f5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "645893f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=img_tensor.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * img_tensor.shape[0], 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        features = YOLO(img_tensor, training=True)\n",
    "        \n",
    "        # extract the features from YOLO giving us a vector of shape (batch, 3, 5, 1024)\n",
    "        # squash that to a shape of (batch, 10, 1024)\n",
    "        features = tf.reshape(features, (features.shape[0], -1, features.shape[3]))\n",
    "        \n",
    "        features = encoder(features)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            # target[:, i]: 取得目標序列（target）在時間步 i 的實際目標值\n",
    "            # 將這個實際目標值轉換為形狀為 (batch_size, 1)，表示模型當前時間i+1的輸入(or i的輸出)\n",
    "            dec_input = tf.expand_dims(target[:, i], 1) \n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = YOLO.trainable_variables + encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3219608a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img_tensor):\n",
    "    img_tensor = img_tensor.numpy()\n",
    "    batch_size = img_tensor.shape[0]\n",
    "    # 重置decoder的初始狀態\n",
    "    hidden = decoder.reset_state(batch_size=batch_size)\n",
    "    \n",
    "    # 建一個(batch_size, 1)的二維tensor，其中每個元素都是<start>對應的索引(也就是2)\n",
    "    # 二維的原因: 第一維表示time step，第二維表示batch size\n",
    "    # 當作decoder的初始輸入，用於生成描述圖像的第一個單字。\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "    \n",
    "    # 儲存預測的單字索引\n",
    "    # shape:(batch_size, time_steps)\n",
    "    result = tf.expand_dims([tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "    \n",
    "    # Extract圖片特徵\n",
    "    features = YOLO(img_tensor)\n",
    "    features = tf.reshape(features, (features.shape[0], -1, features.shape[3]))\n",
    "    features = encoder(features)\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        # decoder生成預測\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "        \n",
    "        # 選擇機率最高的預測單字的索引，返回(batch_size,)的tensor\n",
    "        predicted_id = tf.argmax(predictions, axis=1).numpy() \n",
    "        # predicted_id = tf.argmax(predictions[0]).numpy() # 在第一個timestep預測機率最高的單字跟上一行不同\n",
    "        \n",
    "        # 更新decoder input，準備下一個timestep的輸入\n",
    "        dec_input = tf.expand_dims(predicted_id, 1)\n",
    "\n",
    "        # 將預測結果加入result(batch_size, time_steps)\n",
    "        result = tf.concat([result, predicted_id.reshape((batch_size, 1))], axis=1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "894bb14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_caps_vector_to_word(caps):\n",
    "    '''\n",
    "    將單字的索引轉換為原本的單字\n",
    "    '''\n",
    "    caps = caps.numpy()\n",
    "    capword_list = []\n",
    "    for cap in caps: \n",
    "        capword = ''\n",
    "        for token_num in cap[1:]: #去掉<start>\n",
    "            if token_num == tokenizer.word_index['<end>']:\n",
    "                break\n",
    "            capword += tokenizer.index_word[token_num] # 9 -> t\n",
    "        capword_list.append(capword)\n",
    "    return capword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5079b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataset_valid):\n",
    "    correct = 0\n",
    "    cnt = 0\n",
    "    for imgs, caps in dataset_valid:\n",
    "        prediction_list = batch_caps_vector_to_word(predict(imgs))\n",
    "        ground_truth_list = batch_caps_vector_to_word(caps)\n",
    "        for i, _ in enumerate(prediction_list):\n",
    "            cnt += 1\n",
    "            if prediction_list[i] == ground_truth_list[i]:\n",
    "                correct += 1\n",
    "\n",
    "    return correct / cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3cf9070",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 1000/1000 [05:50<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 1.333528\n",
      "Valid acc: 0.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  2: 100%|██████████| 1000/1000 [07:09<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss 0.248810\n",
      "Valid acc: 0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  3: 100%|██████████| 1000/1000 [05:16<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss 0.061757\n",
      "Valid acc: 0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  4: 100%|██████████| 1000/1000 [04:50<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss 0.028782\n",
      "Valid acc: 0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  5: 100%|██████████| 1000/1000 [04:49<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss 0.028599\n",
      "Valid acc: 0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  6: 100%|██████████| 1000/1000 [04:49<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss 0.014117\n",
      "Valid acc: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  7: 100%|██████████| 1000/1000 [04:49<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss 0.016998\n",
      "Valid acc: 0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  8: 100%|██████████| 1000/1000 [04:49<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss 0.010945\n",
      "Valid acc: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  9: 100%|██████████| 1000/1000 [04:49<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss 0.011986\n",
      "Valid acc: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 1000/1000 [05:01<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss 0.013844\n",
      "Valid acc: 0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 1000/1000 [05:24<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Loss 0.007716\n",
      "Valid acc: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 1000/1000 [04:59<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Loss 0.009331\n",
      "Valid acc: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 1000/1000 [04:57<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Loss 0.005908\n",
      "Valid acc: 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 1000/1000 [04:57<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Loss 0.006485\n",
      "Valid acc: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 1000/1000 [04:57<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Loss 0.007891\n",
      "Valid acc: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 1000/1000 [04:57<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Loss 0.004958\n",
      "Valid acc: 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 1000/1000 [05:01<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Loss 0.006772\n",
      "Valid acc: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 1000/1000 [05:35<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Loss 0.003774\n",
      "Valid acc: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 1000/1000 [05:04<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Loss 0.004525\n",
      "Valid acc: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 1000/1000 [05:23<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Loss 0.004953\n",
      "Valid acc: 0.98\n",
      "Time taken for 20 epoch 6908.685414552689 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "start = time.time()\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    \n",
    "    total_loss = 0\n",
    "\n",
    "    pbar = tqdm(dataset_train, total=num_steps, desc=f'Epoch {epoch + 1:2d}')\n",
    "    \n",
    "    for (batch, (img_tensor, target)) in enumerate(pbar):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "#         if batch % 100 == 0:\n",
    "#             print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "#               epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "    ckpt_manager.save()\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1, total_loss/num_steps))\n",
    "    \n",
    "    score = evaluate(dataset_valid)\n",
    "    print(f'Valid acc: {score:.2f}')\n",
    "    \n",
    "print ('Time taken for {} epoch {} sec\\n'.format(EPOCHS - start_epoch, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5811295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5110d57",
   "metadata": {},
   "source": [
    "## Predict dataset_test(img_name_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc274e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x1af8ce83430>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = tf.train.Checkpoint(feature_extractor=YOLO,\n",
    "                           encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer=optimizer)\n",
    "ckpt.restore('./checkpoints/YOLO/ckpt-20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f00e928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid acc: 0.98\n"
     ]
    }
   ],
   "source": [
    "score = evaluate(dataset_valid)\n",
    "print(f'Valid acc: {score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75f8d6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, IMAGE_SIZE)\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5fef7ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = tf.data.Dataset.from_tensor_slices((img_name_test))\\\n",
    "                              .map(read_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "                              .batch(BATCH_SIZE)\\\n",
    "                              .prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d9a7ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:46<00:00,  4.31it/s]\n"
     ]
    }
   ],
   "source": [
    "output_file = './Lab12-2_110062619.txt'\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    for step, (img_tensor, img_path) in enumerate(tqdm(dataset_test)):\n",
    "        prediction_list = batch_caps_vector_to_word(predict(img_tensor))\n",
    "        for idx, path in enumerate(img_path):\n",
    "            path = path.numpy().decode('utf-8')\n",
    "            name_without_extension = os.path.splitext(os.path.basename(path))[0]\n",
    "            f.write(f'{name_without_extension} {prediction_list[idx]}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
