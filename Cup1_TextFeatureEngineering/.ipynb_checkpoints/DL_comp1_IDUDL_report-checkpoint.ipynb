{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "744e98f8",
   "metadata": {},
   "source": [
    "# DataLab Cup 1: Text Feature Engineering\n",
    "\n",
    "Team name: IDUDL\n",
    "Team members: 110062619 楊淨富 112062611 吳明真 112138502 陳炫妙"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62341323",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b198c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dataset format\n",
    "# train.csv contains 27643 data points (news articles) with the attributes 'Id', 'Page content', and 'Popularity'\n",
    "# test.csv contains 11847 data points with the attributes 'Id' and 'Page content'\n",
    "\n",
    "train  = pd.read_csv('./dataset/train.csv')\n",
    "test  = pd.read_csv('./dataset/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caac2b2b",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "### Functions for Feature Extraction\n",
    "\n",
    "* author:文章作者(去掉By/by)\n",
    "* time:可再細分成Year/Month/Day/Hour/Minute/Second\n",
    "* weekday:星期幾\n",
    "* num_image:文章中圖片的數量\n",
    "* num_link:文章中超連結的數量\n",
    "* len_title:標題的長度(有幾個英文單字)\n",
    "* len_article:文章的長度(有幾個英文單字)\n",
    "* delta_days:抓取發文日期距離年底的天數\n",
    "* article:文章的實際內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bab41f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from bs4 import BeautifulSoup\n",
    "# from datetime import datetime\n",
    "\n",
    "# def author_extract(text):\n",
    "#     soup = BeautifulSoup(text, 'html.parser')\n",
    "#     return(soup.find(attrs = {'class': 'author_name'}).get_text().replace('By','').replace('by','').strip())\n",
    "\n",
    "# def time_extract(text):\n",
    "#     soup = BeautifulSoup(text,\"html.parser\")\n",
    "#     return(soup.find('time').get_text())\n",
    "\n",
    "# def datetime_extract(text):\n",
    "#     # Find date and time \n",
    "#     soup = BeautifulSoup(text, 'html.parser')\n",
    "#     date_time = soup.find('time').get_text()\n",
    "#     if date_time != None:\n",
    "#         weekday = (int(pd.Timestamp(date_time.split(' ')[0]).dayofweek)) # 0: Monday, 1:Tuesday, ...\n",
    "#     else:\n",
    "#         weekday = 7 # No publish date\n",
    "#         date_time = '2013-06-19 15:04:30 UTC'\n",
    "\n",
    "#     # Define a regular expression pattern to match the updated date and time format\n",
    "#     date_pattern = r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2} UTC)'\n",
    "\n",
    "#     # Use re.search to find and capture the date and time\n",
    "#     match_obj = re.search(date_pattern, date_time)\n",
    "\n",
    "#     if match_obj:\n",
    "#         date_time = match_obj.group(1)\n",
    "\n",
    "#     # Split the date and time components\n",
    "#     date_components = date_time.split(' ')\n",
    "#     date, time = date_components[0], date_components[1]\n",
    "\n",
    "#     # Extract individual date and time elements\n",
    "#     date_parts = date.split('-')\n",
    "#     year, month, day = date_parts[0], date_parts[1], date_parts[2]\n",
    "\n",
    "#     time_parts = time.split(':')\n",
    "#     hour, minute, second = time_parts[0], time_parts[1], time_parts[2]\n",
    "#     return year, month, day, weekday, hour, minute, second\n",
    "\n",
    "# def find_weekend(text):\n",
    "#     date_str = time_extract(text).split(' ')[0]\n",
    "#     if date_str == '':\n",
    "#         return(7)\n",
    "#     date = pd.Timestamp(date_str)\n",
    "#     weekday = date.dayofweek\n",
    "#     return(int(weekday))\n",
    "\n",
    "# def num_image(text):\n",
    "#     soup = BeautifulSoup(text, 'html.parser')\n",
    "#     return len(soup.findAll('img'))\n",
    "\n",
    "# def num_link(text):\n",
    "#     soup = BeautifulSoup(text, 'html.parser')\n",
    "#     return len(soup.body.find_all('a'))\n",
    "\n",
    "# def len_title(text):\n",
    "#     soup = BeautifulSoup(text,\"html.parser\")\n",
    "#     title_list = soup.find('h1').get_text()\n",
    "#     return len(title_list)\n",
    "\n",
    "# def len_article(text):\n",
    "#     soup = BeautifulSoup(text,\"html.parser\")\n",
    "#     content = soup.body.find('section', {'class': 'article-content'}).get_text()\n",
    "#     return len(content)\n",
    "\n",
    "# def day_delta(text):\n",
    "#     time_str = time_extract(text)\n",
    "#     if time_str == '':\n",
    "#         return 0\n",
    "#     time_arr = time_str.split(' ')\n",
    "\n",
    "#     year = time_arr[0].split('-')[0]\n",
    "#     target_str = year + '-12-31 23:59:59'\n",
    "#     target = datetime.strptime(target_str, '%Y-%m-%d %H:%M:%S')\n",
    "#     day = datetime.strptime(time_arr[0] + ' ' + time_arr[1], '%Y-%m-%d %H:%M:%S')\n",
    "#     delta = (target - day).days\n",
    "#     return delta\n",
    "\n",
    "# def article_extract(text):\n",
    "#     # remove HTML tags\n",
    "#     soup = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "#     paragraphs = soup.find_all('p')\n",
    "#     article = []\n",
    "#     for p in paragraphs:\n",
    "        \n",
    "#         text = p.get_text()\n",
    "\n",
    "#         # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "#         r = r'(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "#         emoticons = re.findall(r, text)\n",
    "#         text = re.sub(r, '', text)\n",
    "\n",
    "#         # replace('-','') removes nose of emoticons\n",
    "#         text = re.sub(r'[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', '')\n",
    "\n",
    "#         # Output the text\n",
    "#         article.append(text.strip())\n",
    "#     article_str = ''.join(article)\n",
    "\n",
    "#     return article_str\n",
    "\n",
    "# # Example usage\n",
    "# print('[Author]:', author_extract(train['Page content'][1]))\n",
    "# print('[Datetime]:', time_extract(train['Page content'][1]))\n",
    "# print('[Weekday]:', find_weekend(train['Page content'][1]))\n",
    "# print('[# images]:', num_image(train['Page content'][1]))\n",
    "# print('[Length of titles]:', len_title(train['Page content'][1]))\n",
    "# print('[Length of articles]:', len_article(train['Page content'][1]))\n",
    "# print('[Article]:', article_extract(train['Page content'][1]))\n",
    "# print('[Days to the last day]:', day_delta(train['Page content'][1]))\n",
    "# print('[DAY]', datetime_extract(train['Page content'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cb46c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "def preprocessor(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    \n",
    "    # Find the author\n",
    "    article_info = soup.find('div', {'class': 'article-info'})\n",
    "    author_name = soup.find(attrs={'class': 'author_name'})\n",
    "    if author_name:\n",
    "        author = author_name.get_text().replace('By', '').replace('by', '').strip()\n",
    "    elif article_info.span:\n",
    "        author = article_info.span.string\n",
    "    else:\n",
    "        author = article_info.a.string\n",
    "    \n",
    "    author = author.lower()\n",
    "    \n",
    "    # Find weekday\n",
    "    date_time = soup.find('time').get_text()\n",
    "    if date_time != None:\n",
    "        date = pd.Timestamp(date_time.split(' ')[0])\n",
    "        if pd.notna(date_time):\n",
    "            weekday = date.dayofweek # 0: Monday, 1:Tuesday, ...\n",
    "        else:\n",
    "            weekday = 7\n",
    "    else:\n",
    "        weekday = 7 # No publish date\n",
    "        date_time = '2013-06-19 15:04:30 UTC'\n",
    "\n",
    "    # Define a regular expression pattern to match the updated date and time format\n",
    "    date_pattern = r'((\\d{4})-(\\d{2})-(\\d{2}) (\\d{2}):(\\d{2}):(\\d{2}) UTC)'\n",
    "\n",
    "    # Use re.search to find and capture the date and time\n",
    "    match_obj = re.search(date_pattern, date_time)\n",
    "    if match_obj:\n",
    "        year, month, day, hour, minute, second = match_obj.groups()[1:7]\n",
    "    else:\n",
    "        year, month, day, hour, minute, second = 2013,6,19,15,4,30\n",
    "        \n",
    "#     # Split the date and time components\n",
    "#     date_components = date_time.split(' ')\n",
    "#     date, time = date_components[0], date_components[1]\n",
    "\n",
    "#     # Extract individual date and time elements\n",
    "#     date_parts = date.split('-')\n",
    "#     year, month, day = date_parts[0], date_parts[1], date_parts[2]\n",
    "\n",
    "#     time_parts = time.split(':')\n",
    "#     hour, minute, second = time_parts[0], time_parts[1], time_parts[2]\n",
    "    \n",
    "    # Find the topic\n",
    "#     topic_element = soup.find(attrs={'class': 'article-topics'})\n",
    "#     topic = topic_element.get_text().replace('Topics', '').replace(':', '').replace(',', '').strip().lower() if topic_element else ''\n",
    "    a_list = soup.body.find('footer', {'class': 'article-topics'}).find_all('a')\n",
    "    topic_list = [a.string.strip().lower() for a in a_list]\n",
    "    topic = ' '.join([re.sub(r'\\s+', '_', t) for t in topic_list])\n",
    "    \n",
    "    \n",
    "    # Find the title\n",
    "    title = soup.body.h1.string.strip().lower()\n",
    "    \n",
    "    # Find the # of images\n",
    "    num_image = len(soup.body.find_all('img'))\n",
    "    \n",
    "    # Find the # of links\n",
    "    num_link = len(soup.body.find_all('a'))\n",
    "    \n",
    "    # Find the len of title\n",
    "    len_title = len(soup.find('h1').get_text().split())\n",
    "    \n",
    "    # Find the Len of article\n",
    "    len_article = len(soup.body.find('section', {'class': 'article-content'}).get_text().split())\n",
    "    \n",
    "    # Fine the delta days to the last day of the year\n",
    "    time_str = soup.find('time').get_text()\n",
    "    if time_str == '':\n",
    "        delta = 0\n",
    "    else:\n",
    "        time_arr = time_str.split(' ')\n",
    "\n",
    "        year = time_arr[0].split('-')[0]\n",
    "        target_str = year + '-12-31 23:59:59'\n",
    "        target = datetime.strptime(target_str, '%Y-%m-%d %H:%M:%S')\n",
    "        day_format = datetime.strptime(time_arr[0] + ' ' + time_arr[1], '%Y-%m-%d %H:%M:%S')\n",
    "        delta = (target - day_format).days\n",
    "    \n",
    "    return author, year, month, day, weekday, hour, minute, second, topic, \\\n",
    "            title, num_image, num_link, len_title, len_article, delta\n",
    "\n",
    "# feature_list = []\n",
    "# feature_list.append(preprocessor(train['Page content'][0]))\n",
    "\n",
    "feature_list = []\n",
    "for text in train['Page content']:\n",
    "    feature_list.append(preprocessor(text))\n",
    "                      \n",
    "for text in test['Page content']:\n",
    "    feature_list.append(preprocessor(text))\n",
    "    \n",
    "df_extract = pd.DataFrame(\n",
    "    feature_list,\n",
    "    columns=['Author', 'Year', 'Month', 'Day', 'Weekday','Hour', 'Minute', 'Second', 'Topic',\\\n",
    "             'Title', 'Num image', 'Num link', 'Len title', 'Len article', 'Delta']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b92e5dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>Second</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Title</th>\n",
       "      <th>Num image</th>\n",
       "      <th>Num link</th>\n",
       "      <th>Len title</th>\n",
       "      <th>Len article</th>\n",
       "      <th>Delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clara moskowitz</td>\n",
       "      <td>2013</td>\n",
       "      <td>06</td>\n",
       "      <td>19</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15</td>\n",
       "      <td>04</td>\n",
       "      <td>30</td>\n",
       "      <td>asteroid asteroids challenge earth space u.s. ...</td>\n",
       "      <td>nasa's grand challenge: stop asteroids from de...</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>577</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>christina warren</td>\n",
       "      <td>2013</td>\n",
       "      <td>03</td>\n",
       "      <td>28</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "      <td>55</td>\n",
       "      <td>apps_and_software google open_source opn_pledg...</td>\n",
       "      <td>google's new open source patent pledge: we won...</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>305</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sam laird</td>\n",
       "      <td>2014</td>\n",
       "      <td>05</td>\n",
       "      <td>07</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>entertainment nfl nfl_draft sports television</td>\n",
       "      <td>ballin': 2014 nfl draft picks get to choose th...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>1114</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sam laird</td>\n",
       "      <td>2013</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>4.0</td>\n",
       "      <td>02</td>\n",
       "      <td>26</td>\n",
       "      <td>50</td>\n",
       "      <td>sports video videos watercooler</td>\n",
       "      <td>cameraperson fails deliver slapstick laughs</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>278</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>connor finnegan</td>\n",
       "      <td>2014</td>\n",
       "      <td>04</td>\n",
       "      <td>17</td>\n",
       "      <td>3.0</td>\n",
       "      <td>03</td>\n",
       "      <td>31</td>\n",
       "      <td>43</td>\n",
       "      <td>entertainment instagram instagram_video nfl sp...</td>\n",
       "      <td>nfl star helps young fan prove friendship with...</td>\n",
       "      <td>51</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>1370</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Author  Year Month Day  Weekday Hour Minute Second  \\\n",
       "0   clara moskowitz  2013    06  19      2.0   15     04     30   \n",
       "1  christina warren  2013    03  28      3.0   17     40     55   \n",
       "2         sam laird  2014    05  07      2.0   19     15     20   \n",
       "3         sam laird  2013    10  11      4.0   02     26     50   \n",
       "4   connor finnegan  2014    04  17      3.0   03     31     43   \n",
       "\n",
       "                                               Topic  \\\n",
       "0  asteroid asteroids challenge earth space u.s. ...   \n",
       "1  apps_and_software google open_source opn_pledg...   \n",
       "2      entertainment nfl nfl_draft sports television   \n",
       "3                    sports video videos watercooler   \n",
       "4  entertainment instagram instagram_video nfl sp...   \n",
       "\n",
       "                                               Title  Num image  Num link  \\\n",
       "0  nasa's grand challenge: stop asteroids from de...          1        21   \n",
       "1  google's new open source patent pledge: we won...          1        16   \n",
       "2  ballin': 2014 nfl draft picks get to choose th...          1         9   \n",
       "3        cameraperson fails deliver slapstick laughs          0        11   \n",
       "4  nfl star helps young fan prove friendship with...         51        14   \n",
       "\n",
       "   Len title  Len article  Delta  \n",
       "0          8          577    195  \n",
       "1         12          305    278  \n",
       "2         12         1114    238  \n",
       "3          5          278     81  \n",
       "4         10         1370    258  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_extract.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddf55e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df_extract.copy()\n",
    "df_drop = df_copy.drop(columns=['Minute', 'Second'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfc403bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Title</th>\n",
       "      <th>Num image</th>\n",
       "      <th>Num link</th>\n",
       "      <th>Len title</th>\n",
       "      <th>Len article</th>\n",
       "      <th>Delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clara moskowitz</td>\n",
       "      <td>2013</td>\n",
       "      <td>06</td>\n",
       "      <td>19</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15</td>\n",
       "      <td>asteroid asteroids challenge earth space u.s. ...</td>\n",
       "      <td>nasa's grand challenge: stop asteroids from de...</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>577</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>christina warren</td>\n",
       "      <td>2013</td>\n",
       "      <td>03</td>\n",
       "      <td>28</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17</td>\n",
       "      <td>apps_and_software google open_source opn_pledg...</td>\n",
       "      <td>google's new open source patent pledge: we won...</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>305</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sam laird</td>\n",
       "      <td>2014</td>\n",
       "      <td>05</td>\n",
       "      <td>07</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19</td>\n",
       "      <td>entertainment nfl nfl_draft sports television</td>\n",
       "      <td>ballin': 2014 nfl draft picks get to choose th...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>1114</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sam laird</td>\n",
       "      <td>2013</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>4.0</td>\n",
       "      <td>02</td>\n",
       "      <td>sports video videos watercooler</td>\n",
       "      <td>cameraperson fails deliver slapstick laughs</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>278</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>connor finnegan</td>\n",
       "      <td>2014</td>\n",
       "      <td>04</td>\n",
       "      <td>17</td>\n",
       "      <td>3.0</td>\n",
       "      <td>03</td>\n",
       "      <td>entertainment instagram instagram_video nfl sp...</td>\n",
       "      <td>nfl star helps young fan prove friendship with...</td>\n",
       "      <td>51</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>1370</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Author  Year Month Day  Weekday Hour  \\\n",
       "0   clara moskowitz  2013    06  19      2.0   15   \n",
       "1  christina warren  2013    03  28      3.0   17   \n",
       "2         sam laird  2014    05  07      2.0   19   \n",
       "3         sam laird  2013    10  11      4.0   02   \n",
       "4   connor finnegan  2014    04  17      3.0   03   \n",
       "\n",
       "                                               Topic  \\\n",
       "0  asteroid asteroids challenge earth space u.s. ...   \n",
       "1  apps_and_software google open_source opn_pledg...   \n",
       "2      entertainment nfl nfl_draft sports television   \n",
       "3                    sports video videos watercooler   \n",
       "4  entertainment instagram instagram_video nfl sp...   \n",
       "\n",
       "                                               Title  Num image  Num link  \\\n",
       "0  nasa's grand challenge: stop asteroids from de...          1        21   \n",
       "1  google's new open source patent pledge: we won...          1        16   \n",
       "2  ballin': 2014 nfl draft picks get to choose th...          1         9   \n",
       "3        cameraperson fails deliver slapstick laughs          0        11   \n",
       "4  nfl star helps young fan prove friendship with...         51        14   \n",
       "\n",
       "   Len title  Len article  Delta  \n",
       "0          8          577    195  \n",
       "1         12          305    278  \n",
       "2         12         1114    238  \n",
       "3          5          278     81  \n",
       "4         10         1370    258  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_drop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7afbf3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer(text):\n",
    "    if type(text) == np.ndarray:\n",
    "        text = text[0]\n",
    "    return re.split(r'\\s+', text.strip())\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter= PorterStemmer()\n",
    "    \n",
    "    if type(text) == np.ndarray:\n",
    "        text = text[0]\n",
    "    text = re.sub(r\"([\\w]+)'[\\w]+\", (lambda match_obj: match_obj.group(1)), text) # it's -> its\n",
    "    text = re.sub(r'\\.', '', text)\n",
    "    text = re.sub(r'[^\\w]+', ' ', text)\n",
    "    \n",
    "    return [porter.stem(w) for w in re.split(r'\\s+', text.strip()) \\\n",
    "           if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "def tokenizer_lem(text):\n",
    "    if type(text) == np.ndarray:\n",
    "        text = text[0]\n",
    "    text = re.sub(r\"([\\w]+)'[\\w]+\", (lambda match_obj: match_obj.group(1)), text)\n",
    "    text = re.sub(r'\\.', '', text)\n",
    "    text = re.sub(r'[^\\w]+', ' ', text)\n",
    "    wnl = WordNetLemmatizer()\n",
    "    return [wnl.lemmatize(w, pos=\"v\") for w in re.split(r'\\s+', text.strip())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6e0ddf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "trans_att = ColumnTransformer(\n",
    "    [('Author', CountVectorizer(tokenizer=tokenizer, lowercase=False), [0]),\n",
    "     ('Topic', CountVectorizer(tokenizer=tokenizer_stem_nostop, lowercase=False), [6]),\n",
    "     ('Title', CountVectorizer(tokenizer=tokenizer_stem_nostop, lowercase=False), [7])],\n",
    "    n_jobs=-1,\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "trans_tt = ColumnTransformer(\n",
    "    [('Author', 'drop', [0]),\n",
    "     ('Topic', CountVectorizer(tokenizer=tokenizer_stem_nostop, lowercase=False), [6]),\n",
    "     ('Title', CountVectorizer(tokenizer=tokenizer_stem_nostop, lowercase=False), [7])],\n",
    "    n_jobs=-1,\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ca05529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_train_all = df_drop.values[:train.shape[0]] # [0:27643]\n",
    "y_train_all = (train['Popularity'].values == 1).astype(int)\n",
    "X_test = df_drop.values[train.shape[0]:] # [27643:]\n",
    "\n",
    "columns_to_scale = [1,2,3,4,5,8,9,10,11,12]\n",
    "X_train_all[:, columns_to_scale] = sc.fit_transform(X_train_all[:, columns_to_scale])\n",
    "X_test[:, columns_to_scale] = sc.transform(X_test[:, columns_to_scale])\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_all, y_train_all, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31d64460",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auc (10-fold cv)]\n",
      "LGBMClassifier: 0.6056 (+/-0.0157)\n",
      "train scroe: 0.6817\n",
      "valid score: 0.5926\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "lgbm = Pipeline([('ct', trans_tt),\n",
    "                 ('clf', LGBMClassifier(force_row_wise=True, random_state=0, learning_rate=0.005, n_estimators=500, verbose=-1))])\n",
    "\n",
    "# CV\n",
    "# 對完整的訓練數據進行訓練\n",
    "print('[auc (10-fold cv)]')\n",
    "scores = cross_val_score(estimator=lgbm, X=X_train_all, y=y_train_all, cv=10, scoring='roc_auc')\n",
    "print(f'LGBMClassifier: {scores.mean():.4f} (+/-{scores.std():.4f})')\n",
    "\n",
    "# 同時獲取訓練集的分數和估計器實例\n",
    "# scores = cross_validate(estimator=clf, X=X_train_all, y=y_train_all, cv =10,scoring='roc_auc', \\\n",
    "#                             return_train_score=True, return_estimator=True)\n",
    "# print(f\"train score: {np.mean(scores['train_score']):.4f} (+/-{np.std(scores['train_score']):.4f}\")\n",
    "# print(f\"valid score: {np.mean(scores['test_score']):.4f} (+/-{np.std(scores['test_score']):.4f}\")\n",
    "\n",
    "# 切成訓練集和驗證集分數\n",
    "lgbm.fit(X_train, y_train)\n",
    "print(f'train scroe: {roc_auc_score(y_train, lgbm.predict_proba(X_train)[:, 1]):.4f}')\n",
    "print(f'valid score: {roc_auc_score(y_valid, lgbm.predict_proba(X_valid)[:, 1]):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14be0bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auc (10-fold cv)]\n",
      "RandomForestClassifier: 0.5936 (+/-0.0155)\n",
      "train scroe: 1.0000\n",
      "valid score: 0.5884\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "rf = Pipeline([('ct', trans_tt),\n",
    "                ('clf', RandomForestClassifier(n_jobs=-1, random_state=0, n_estimators=300))])\n",
    "\n",
    "print('[auc (10-fold cv)]')\n",
    "scores = cross_val_score(estimator=rf, X=X_train_all, y=y_train_all, cv=10, scoring='roc_auc')\n",
    "print(f'RandomForestClassifier: {scores.mean():.4f} (+/-{scores.std():.4f})')\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "print(f'train scroe: {roc_auc_score(y_train, rf.predict_proba(X_train)[:, 1]):.4f}')\n",
    "print(f'valid score: {roc_auc_score(y_valid, rf.predict_proba(X_valid)[:, 1]):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ed444ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auc (10-fold cv)]\n",
      "XGBoostClassifier: 0.5747 (+/-0.0149)\n",
      "train scroe: 0.9219\n",
      "valid score: 0.5635\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "xgb = Pipeline([('ct', trans_tt),\n",
    "                ('clf', XGBClassifier(verbosity=0, n_estimators=500))])\n",
    "\n",
    "print('[auc (10-fold cv)]')\n",
    "scores = cross_val_score(estimator=xgb, X=X_train_all, y=y_train_all, cv=10, scoring='roc_auc')\n",
    "print(f'XGBoostClassifier: {scores.mean():.4f} (+/-{scores.std():.4f})')\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "print(f'train scroe: {roc_auc_score(y_train, xgb.predict_proba(X_train)[:, 1]):.4f}')\n",
    "print(f'valid score: {roc_auc_score(y_valid, xgb.predict_proba(X_valid)[:, 1]):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57273b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf = lgbm\n",
    "\n",
    "y_score = best_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "test_pred = pd.DataFrame({'Id': test['Id'], 'Popularity': y_score})\n",
    "test_pred.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
